{"cells":[{"cell_type":"markdown","metadata":{"id":"k5mRDji1iNsc"},"source":["# Fall 2022: DS-GA 1011 NLP with Representation Learning\n","## Homework 2\n","## Part 3: Neural Machine Translation (30 pts)\n","In this part, you implement Transformer encoder for Neural Machine Translation (NMT) using a sequence to sequence (seq2seq) model for English to French translation with PyTorch."]},{"cell_type":"markdown","metadata":{"id":"ki8Rdu4IiNsd"},"source":["---\n","### 1 Transformer Encoder (18 pts)"]},{"cell_type":"code","execution_count":106,"metadata":{"id":"OB4991PPiNse"},"outputs":[],"source":["# Add utilities path\n","import sys\n","\n","path_to_utils = 'utils'\n","sys.path.append(path_to_utils)"]},{"cell_type":"code","execution_count":107,"metadata":{"id":"1h61HxKEiNsi"},"outputs":[],"source":["# Import modules\n","import time\n","from tqdm import notebook\n","from functools import partial\n","\n","import torch\n","from torch import optim\n","import torch.nn as nn\n","\n","import global_variables\n","import nmt_dataset\n","import nnet_models_new"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"juHsWAmriNsl"},"outputs":[],"source":["# Load data\n","import os\n","\n","source_name = 'en'\n","target_name = 'fr'\n","\n","base_saved_models_dir = '.'\n","saved_models_dir = os.path.join(base_saved_models_dir, source_name+'2'+target_name)\n","\n","main_data_path = './data/'\n","\n","path_to_train_data = {'source':main_data_path+'train.'+source_name, \n","                      'target':main_data_path+'train.'+target_name}\n","path_to_val_data = {'source': main_data_path+'valid.'+source_name, \n","                      'target':main_data_path+'valid.'+target_name}"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["i think we may have something that you d be interested in buying .\n","they got it .\n","i m glad to see you .\n","he got into his car in a hurry .\n","do you like mozart s music ?\n"]}],"source":["! head -5 './data/train.en'"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"4ZISJFayiNso"},"outputs":[],"source":["saved_language_model_dir = os.path.join(saved_models_dir, 'lang_obj')\n","\n","dataset_dict = {'train': nmt_dataset.LanguagePair(source_name = source_name, target_name=target_name, \n","                    filepath = path_to_train_data, \n","                    lang_obj_path = saved_language_model_dir,\n","                     minimum_count = 1), \n","\n","                'val': nmt_dataset.LanguagePair(source_name = source_name, target_name=target_name, \n","                    filepath = path_to_val_data, \n","                    lang_obj_path = saved_language_model_dir,\n","                    minimum_count = 1)}"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"9F04tskaiNsr"},"outputs":[{"name":"stdout","output_type":"stream","text":["MAX_LEN: 32\n"]}],"source":["MAX_LEN = int(dataset_dict['train'].main_df['source_len'].quantile(0.9999))\n","batchSize = 64\n","print('MAX_LEN:', MAX_LEN)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"-5GP1oyqiNsv"},"outputs":[],"source":["from functools import partial\n","from torch.utils.data import DataLoader\n","\n","dataloader_dict = {'train': DataLoader(dataset_dict['train'], batch_size = batchSize, \n","                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=MAX_LEN),\n","                            shuffle = True, num_workers=0), \n","                    'val': DataLoader(dataset_dict['val'], batch_size = batchSize, \n","                            collate_fn = partial(nmt_dataset.vocab_collate_func, MAX_LEN=MAX_LEN),\n","                            shuffle = True, num_workers=0) }"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"l_T7Fi87iNsy"},"outputs":[],"source":["# Configuration\n","source_lang_obj = dataset_dict['train'].source_lang_obj\n","target_lang_obj = dataset_dict['train'].target_lang_obj\n","\n","source_vocab = dataset_dict['train'].source_lang_obj.n_words\n","target_vocab = dataset_dict['train'].target_lang_obj.n_words\n","hidden_size = 512\n","enc_layers = 1\n","lr = 0.25\n","longest_label = 1\n","gradient_clip = 0.3\n","use_cuda = True\n","\n","num_epochs = 20"]},{"cell_type":"markdown","metadata":{"id":"0bghnW3YiNs2"},"source":["#### 1.1 Encoder (9 pts)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["import math\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, d_model, dropout, max_len=100):\n","        \"\"\"Initialize positional encoder.\n","        :param d_model: hidden size of desired model\n","        :param dropout: pct of embeddings to randomly zero out\n","        :param max_len: maximum sentence length in batch\n","\n","        Note that d_model is the same as model hidden size because embeddings need to be added together\n","        Implementation from http://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks\n","        \"\"\"\n","        \n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        pe = torch.zeros(max_len, d_model)\n","\n","        position = torch.arange(0, max_len).unsqueeze(1) # shape = (max_len, 1) for downstream vectorized computation\n","        \n","        # div_term simplifies to 10000^(2i/d_model), but we define it this way to make vectorized computations easier\n","        div_term = torch.exp(\n","            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n","        )\n","        pe[:, 0::2] = torch.sin(position * div_term) # Every 2i in positional embedding is sin(position / 10000^(2i/d_model))\n","        pe[:, 1::2] = torch.cos(position * div_term) # Every 2i+1 in positional embedding is cos(position / 10000^(2i/d_model))\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer(\"pe\", pe)\n","\n","    def forward(self, x):\n","        # Takes x as batched embedding matrix w/ size (batch_size, sentence_length, hidden_size) and adds positional embedding values\n","        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n","        return self.dropout(x)"]},{"cell_type":"code","execution_count":98,"metadata":{"id":"O4T23W8liNs4"},"outputs":[],"source":["# Add transformer as encoder in seq2seq model\n","import math\n","\n","# code below can help you to start it, but feel free to start from scratch\n","class EncoderTransformer(nn.Module):\n","    def __init__(self, options):\n","        \"\"\"Initialize encoder.\n","        :param options: architecture parameters which must include:\n","            - hidden_size: transformer hidden size\n","            - max_len: maximum sentence length\n","            - d_model: input embedding size\n","            - num_heads: number of attn heads\n","            - num_layers: number of transformer layers\n","            - vocab_size: input vocab size (dimension 0 of self.encoder)\n","        \"\"\"\n","\n","        super().__init__()\n","        # you need to add more things here\n","        self.position_encoder = PositionalEncoding(options['d_model'], 0, options['max_len'])\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=options['d_model'], nhead=options['num_heads'], batch_first=True)\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=options['num_layers'])\n","\n","        self.encoder = nn.Embedding(options['vocab_size'], options['d_model'], padding_idx=global_variables.PAD_IDX)\n","        self.d_model = options['d_model']\n","\n","        self.init_weights()\n","    \n","    def init_weights(self) -> None:  \n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, src):\n","        # some helpful directions below, check the MLM lab for more details\n","\n","        embedded = self.encoder(src) * math.sqrt(self.d_model)\n","        embedded_wpos = self.position_encoder(embedded) # add pos embeddings to text embeddings\n","        outputs = self.transformer(embedded_wpos)\n","        hidden = torch.mean(outputs, dim=1).unsqueeze(0)\n","        \n","        return outputs, hidden\n","        "]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["# Architecture parameters\n","hidden_size = 512\n","num_layers = 1\n","\n","t_encoder_options = {\n","    'hidden_size': hidden_size,\n","    'max_len': MAX_LEN,\n","    'd_model': hidden_size,\n","    'num_heads': 2,\n","    'num_layers': num_layers,\n","    'vocab_size': source_vocab,\n","}\n","\n","encoder_t = EncoderTransformer(t_encoder_options)"]},{"cell_type":"markdown","metadata":{"id":"xcUYkz_RiNs9"},"source":["#### 1.2 Decoder(s) (9 pts)"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[],"source":["# Training parameters\n","lr = 0.25\n","longest_label = 1\n","gradient_clip = 0.3\n","use_cuda = True\n","num_epochs = 20"]},{"cell_type":"code","execution_count":101,"metadata":{"id":"OT_JZXWeiNs9"},"outputs":[],"source":["# Basic RNN decoder (no attention)\n","rnn_layers = 1\n","decoder_rnn_basic = nnet_models_new.DecoderRNN(output_size=target_vocab, hidden_size=hidden_size, numlayers=rnn_layers)"]},{"cell_type":"code","execution_count":102,"metadata":{"id":"U6DmOcLViNtB"},"outputs":[],"source":["# RNN Decoder with Encoder attention\n","encoder_attention = True\n","self_attention = False\n","\n","decoder_encoderattn = nnet_models_new.Decoder_SelfAttn(output_size=target_vocab,\n","                                                    hidden_size=hidden_size, \n","                                                    encoder_attention = encoder_attention,\n","                                                    self_attention = self_attention)"]},{"cell_type":"code","execution_count":103,"metadata":{"id":"szrb9vkiiNtE"},"outputs":[],"source":["# RNN Decoder with Encoder & Self attention\n","encoder_attention = True \n","self_attention = True\n","\n","decoder_encoderselfattn = nnet_models_new.Decoder_SelfAttn(output_size=target_vocab,\n","                                                    hidden_size=hidden_size, \n","                                                    encoder_attention = encoder_attention,\n","                                                    self_attention = self_attention)"]},{"cell_type":"markdown","metadata":{"id":"u_dYn8C_iNtH"},"source":["#### Training & Evaluation"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["def get_full_filepath(path, dec_type):\n","    filename = 'nmt_t_enc_'+dec_type+'_dec.pth'\n","    return os.path.join(path, filename)\n","\n","def save_models(nmt_model, path, dec_type):\n","    if not os.path.exists(path):\n","            os.makedirs(path)\n","    filename = 'nmt_t_enc_'+dec_type+'_dec.pth'\n","    torch.save(nmt_model, os.path.join(path, filename))\n","\n","def train_model(dataloader, nmt, num_epochs=50, val_every=1, saved_model_path = 'models/', dec_type ='basic_rnn'):\n","\n","    best_bleu = -1\n","    for epoch in range(num_epochs):\n","\n","        start = time.time()\n","        running_loss = 0\n","\n","        print('Epoch: [{}/{}]'.format(epoch, num_epochs));\n","        \n","        for i, data in notebook.tqdm(enumerate(dataloader['train']), total=len(dataloader['train'])):  \n","            _, curr_loss = nmt.train_step(data);\n","            running_loss += curr_loss\n","\n","        epoch_loss = running_loss / len(dataloader['train']) \n","        \n","        print(\"epoch {} loss = {}, time = {}\".format(epoch, epoch_loss,\n","                                                        time.time() - start))\n","        sys.stdout.flush()\n","   \n","        if epoch%val_every == 0:\n","            val_bleu_score = nmt.get_bleu_score(dataloader['val']);\n","            print('validation bleu: ', val_bleu_score)\n","            sys.stdout.flush()\n","            \n","            nmt.scheduler_step(val_bleu_score);\n","            \n","            if val_bleu_score > best_bleu:\n","                best_bleu = val_bleu_score\n","                save_models(nmt, saved_model_path, dec_type);\n","\n","        print('='*50)\n","\n","    print(\"Training completed. Best BLEU is {}\".format(best_bleu))"]},{"cell_type":"code","execution_count":105,"metadata":{"id":"N1-h6PlgiNtI"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training NMT with transformer encoder and basic_rnn decoder\n","Epoch: [0/20]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2fbcfa04a97e478ab07c8d90a02984d1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1805 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [105], line 26\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     nmt_t_rnn \u001b[38;5;241m=\u001b[39m nnet_models_new\u001b[38;5;241m.\u001b[39mseq2seq(\n\u001b[1;32m     17\u001b[0m                                     encoder_t, decoders[decoder],\n\u001b[1;32m     18\u001b[0m                                     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m                                     clip\u001b[38;5;241m=\u001b[39mgradient_clip\n\u001b[1;32m     24\u001b[0m                                 )\n\u001b[0;32m---> 26\u001b[0m     train_model(dataloader_dict, nmt_t_rnn, \n\u001b[1;32m     27\u001b[0m                 num_epochs \u001b[38;5;241m=\u001b[39m num_epochs, \n\u001b[1;32m     28\u001b[0m                 saved_model_path \u001b[38;5;241m=\u001b[39m saved_model_path, \n\u001b[1;32m     29\u001b[0m                 dec_type \u001b[38;5;241m=\u001b[39m decoder)\n","Cell \u001b[0;32mIn [104], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(dataloader, nmt, num_epochs, val_every, saved_model_path, dec_type)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_epochs));\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m notebook\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])):  \n\u001b[0;32m---> 22\u001b[0m     _, curr_loss \u001b[38;5;241m=\u001b[39m \u001b[43mnmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     23\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m curr_loss\n\u001b[1;32m     25\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n","File \u001b[0;32m~/Documents/NYU_MSDS/nlp_learning/hw2/pyfiles/nnet_models_new.py:418\u001b[0m, in \u001b[0;36mseq2seq.train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    416\u001b[0m scores \u001b[39m=\u001b[39m decoder_output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, decoder_output\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    417\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(scores, ys\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 418\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_params()\n\u001b[1;32m    421\u001b[0m _max_score, predictions \u001b[39m=\u001b[39m decoder_output\u001b[39m.\u001b[39mmax(\u001b[39m2\u001b[39m)\n","File \u001b[0;32m~/.pyenv/versions/capstone/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n","File \u001b[0;32m~/.pyenv/versions/capstone/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Train\n","decoders = {\n","    'basic_rnn': decoder_rnn_basic,\n","    'rnn_encoderattn': decoder_encoderattn,\n","    'rnn_encoderselfattn': decoder_encoderselfattn\n","}\n","\n","train_again = False\n","saved_model_path = 'models/'\n","\n","for decoder in decoders:\n","    print(f'Training NMT with transformer encoder and {decoder} decoder')\n","    if os.path.exists(get_full_filepath(saved_model_path, decoder)) and (not train_again):\n","        nmt_t_rnn = torch.load(get_full_filepath(saved_model_path, decoder), map_location=global_variables.device)\n","    else:\n","        nmt_t_rnn = nnet_models_new.seq2seq(\n","                                        encoder_t, decoders[decoder],\n","                                        lr=lr,\n","                                        use_cuda=use_cuda,\n","                                        hiddensize=hidden_size,\n","                                        target_lang=dataset_dict['train'].target_lang_obj,\n","                                        longest_label=longest_label,\n","                                        clip=gradient_clip\n","                                    )\n","        \n","        train_model(dataloader_dict, nmt_t_rnn, \n","                    num_epochs = num_epochs, \n","                    saved_model_path = saved_model_path, \n","                    dec_type = decoder)"]},{"cell_type":"markdown","metadata":{"id":"HLef3aD5iNtM"},"source":["---\n","### 2 Attention visualization (12 pts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVcysNCtiNtN"},"outputs":[],"source":["# Model was trained in ~2 hours, i.e. you can expect attention maps\n","# to look quite 'hard' (less soft spreading) i.e. attending to some particular token in the input"]}],"metadata":{"colab":{"name":"hw2-part3-nmt.ipynb","provenance":[]},"kernelspec":{"display_name":"capstone","language":"python","name":"capstone"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
